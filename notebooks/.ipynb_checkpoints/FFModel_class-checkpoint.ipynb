{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.utils.optimize import newton_cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-3b11b477970f>, line 94)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-3b11b477970f>\"\u001b[0;36m, line \u001b[0;32m94\u001b[0m\n\u001b[0;31m    yd = #*** replace cubic function with b-splines ***\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def _check_solver(solver, penalty, jac, hess):\n",
    "\n",
    "    all_solvers = ['nelder-mead', 'cg', 'newton-cg']\n",
    "\n",
    "    if solver not in all_solvers:\n",
    "        raise ValueError(\"Force field optimization supports only solvers in %s, got\"\n",
    "                         \" %s.\" % (all_solvers, solver))\n",
    "\n",
    "    all_penalties = ['energy', 'force', 'sd2']\n",
    "    if penalty not in all_penalties:\n",
    "        raise ValueError(\"Force field optimization supports only penalties in %s,\"\n",
    "                         \" got %s.\" % (all_penalties, penalty))\n",
    "\n",
    "    if solver == 'newton-cg' and (not hess or not jac):\n",
    "        raise ValueError(\"Solver %s requires both Jacobian and Hessian\" % (solver))\n",
    "\n",
    "    if solver == 'cg' and not jac:\n",
    "        raise ValueError(\"Solver %s requires Jacobian \" % (solver))\n",
    " \n",
    "    return solver\n",
    "\n",
    "class Hamiltonian:\n",
    "    \"\"\"\n",
    "    Class defining the form of a B-spline-based EAM.\n",
    "    For historical reasons, the first two arguments in w correspond to embedding\n",
    "    functio w*sqrt(rho) + w*rho**2. Then pair interactions and then density functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ndens=0, w=None):\n",
    "        self.ndens = ndens\n",
    "        self.w = w\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.ndens, self.w\n",
    "    \n",
    "    def set_params(ndens=0, w=None):\n",
    "        self.ndens = ndens\n",
    "        self.w = w\n",
    "\n",
    "    def energy(X, w):\n",
    "        \"\"\" Returns energy of a configuration for given parameters.\n",
    "        \"\"\"\n",
    "        energy = 0.0\n",
    "        if self.ndens == 0:\n",
    "            # Linear Hamiltonian\n",
    "            energy += X.dot(w)\n",
    "        else:\n",
    "            # Hamiltonian with interaction terms\n",
    "            # density function\n",
    "            dens = X[-ndens:].dot(w[-ndens:])\n",
    "        \n",
    "            # embedding function\n",
    "            energy += w[0]*np.sqrt(dens)\n",
    "            energy += w[1]*dens**2\n",
    "        \n",
    "            # pair interactions\n",
    "            energy += X[:,2:-ndens].dot(w[2:-ndens])\n",
    "\n",
    "        return energy\n",
    "\n",
    "def _tpf_to_bspline(n_tot):\n",
    "    \"\"\"\n",
    "    Finds b-spline coefficients reproducing a single cubic\n",
    "    functin (rc-r)**3\n",
    "    \"\"\"\n",
    "\n",
    "    penalty = [1.0, -4.0, 6.0, -4.0, 1.0]\n",
    "    o = len(penalty) -1 #order\n",
    "    P = np.zeros((n_tot, n_tot), dtype=float)\n",
    "    for i in range(n_tot):\n",
    "        for j, p in enumerate(penalty):\n",
    "            if i+j-o < 0 or i+j-o > n_tot:\n",
    "                continue\n",
    "            P[i, i+j-o] = p\n",
    "\n",
    "    Pinv = np.linalg.inv(P.T)\n",
    "    on = np.zeros(n_tot)\n",
    "    on[-1] = 1.0\n",
    "    return Pinv.dot(on)\n",
    "\n",
    "def _linear_Hamiltonian_approx(X, y, ham, alpha, DD, CD):\n",
    "    \"\"\"Linear regression solver for energy matching\n",
    "    \n",
    "    Used primarily for setting up initial parameters.\n",
    "    For linear Hamiltonian, the solution is exact.\n",
    "    For non-linear Hamiltonian, a linear approximation with a single cubic truncated\n",
    "    basis function is used to set b-spline parameters for further optimization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w: ndarray, shape (n_feratures,)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ndens = ham.get_params()[0]\n",
    "\n",
    "    if ndens == 0:\n",
    "       # all linear, simply solve a hat matrix equation\n",
    "        w = np.linalg.inv(X.T.dot(X) + alpha*DD).dot(X.T).dot(y)\n",
    "    else:\n",
    "        w = np.empty((X.shape[1],), dtype=float)\n",
    "        # Solve assuming a simple cubic density function\n",
    "        Xl = X[:,:-ndens]\n",
    "        w[:-ndens] = np.linalg.inv(Xl.T.dot(Xl) + alpha*DD).dot(Xl.T).dot(y)\n",
    "        # Set density function b-spline coefficients to reproduce cubic function (r_c - r)^3\n",
    "        # The end of the last bspline has to coincide with r_c of the cubic function\n",
    "        w[-ndens:] = _tpf_to_bspline(ndens)\n",
    " \n",
    "    return w\n",
    "\n",
    "def _energy_loss(w, X, y, alpha, DD, CD, ham, sample_weight=None):\n",
    "    \"\"\"Computes the energy loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_features,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Array of labels.\n",
    "    alpha : float\n",
    "        Regularization parameter. alpha is equal to 1 / C.\n",
    "    DD : ndarray, shape (n_features, n_features)\n",
    "        Regualarization matrix for calculating difference penalty\n",
    "    sample_weight : array-like, shape (n_samples,) optional\n",
    "        Array of weights that are assigned to individual samples.\n",
    "        If not provided, then each sample is given unit weight.\n",
    "    Returns\n",
    "    -------\n",
    "    out : float\n",
    "        Logistic loss.\n",
    "    \"\"\"\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(y.shape[0])\n",
    "\n",
    "    # Energy loss with penalty matrix (linear Hamiltonian (for start))\n",
    "    #dy = y - X.dot(w)\n",
    "    dy = y - ham.energy(X, w)\n",
    "    out = dy.T.dot(dy) + 0.5 * alpha * w.T.dot(DD).dot(w)  \n",
    "\n",
    "    # X.dot(w) will be replaced by Hamiltonian(X, w)\n",
    "    #out = dy.T.dot(dy) + 0.5 * alpha * w.T.dot(DD).dot(w)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _energy_loss_and_grad(w, X, y, alpha, DD, CD, ham, sample_weight=None):\n",
    "    \"\"\"Computes the logistic loss and gradient.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_features,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Array of labels.\n",
    "    alpha : float\n",
    "        Regularization parameter. alpha is equal to 1 / C.\n",
    "    sample_weight : array-like, shape (n_samples,) optional\n",
    "        Array of weights that are assigned to individual samples.\n",
    "        If not provided, then each sample is given unit weight.\n",
    "    Returns\n",
    "    -------\n",
    "    out : float\n",
    "        Energy loss.\n",
    "    grad : ndarray, shape (n_features,)\n",
    "        Energy loss gradient.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    grad = np.empty_like(w)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(n_samples)\n",
    "\n",
    "    # Energy loss\n",
    "    #dy = y - X.dot(w)\n",
    "    dy = y - ham.energy(X, w)\n",
    "    out = dy.T.dot(dy) + 0.5 * alpha * w.T.dot(DD).dot(w) \n",
    "\n",
    "    # Energy grad\n",
    "    grad = -2.0 * X.T.dot(dy) + alpha * DD.dot(w)\n",
    "    #grad = -2.0 * X.T.dot(y - X.dot(w)) + alpha * DD.dot(w)\n",
    "\n",
    "    return out, grad\n",
    "\n",
    "\n",
    "def _energy_grad_hess(w, X, y, alpha, DD, CD, ham, sample_weight=None):\n",
    "    \"\"\"Computes the gradient and the Hessian, in the case of a logistic loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_features,) or (n_features + 1,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Array of labels.\n",
    "    alpha : float\n",
    "        Regularization parameter. alpha is equal to 1 / C.\n",
    "    DD : ndarray, shape (n_features, n_features)\n",
    "        Regualarization matrix for calculating difference penalty\n",
    "    sample_weight : array-like, shape (n_samples,) optional\n",
    "        Array of weights that are assigned to individual samples.\n",
    "        If not provided, then each sample is given unit weight.\n",
    "    Returns\n",
    "    -------\n",
    "    grad : ndarray, shape (n_features,) or (n_features + 1,)\n",
    "        Logistic gradient.\n",
    "    Hs : callable\n",
    "        Function that takes the gradient as a parameter and returns the\n",
    "        matrix product of the Hessian and gradient.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    grad = np.empty_like(w)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(y.shape[0])\n",
    "\n",
    "    dy = y - ham.energy(X, w)\n",
    "    grad = -2.0 * X.T.dot(dy) + alpha * DD.dot(w)\n",
    "\n",
    "    # The mat-vec product of the Hessian and gradient s\n",
    "    def Hs(s):\n",
    "        ret = X.T.dot(X).dot(s) + alpha * DD.dot(s)\n",
    "        return ret\n",
    "\n",
    "    return grad, Hs\n",
    "\n",
    "\n",
    "class FFOptimizer(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    General force field optimizer.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ham : Hamiltonian object\n",
    "        Hamiltonian defining the model potential\n",
    "    penalty : str, 'sd2', 'energy', or 'force', default: 'sd2'\n",
    "        Used to specify the norm used in the penalization. Defaults to sd2,\n",
    "        which is equivalent to statistical (Rao) distance minimization.\n",
    "        'energy' penalty corresponds to least squares energy matching\n",
    "        and 'force' penalty corresponds to least squares force matching\n",
    "    jac : 1D-array, shape (n_features)\n",
    "        Jacobian\n",
    "    hess : 2D-array, shape (n_features, n_features)\n",
    "        Hessian\n",
    "    alpha : {float, array-like}, shape (n_targets)\n",
    "        Regularization strength; must be a positive float. Regularization\n",
    "        improves the conditioning of the problem and reduces the variance of\n",
    "        the estimates. Larger values specify stronger regularization.\n",
    "        Alpha corresponds to ``C^-1`` in other linear models such as\n",
    "        LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
    "        assumed to be specific to the targets. Hence they must correspond in\n",
    "        number.\n",
    "    reg : {2D-array}, shape (n_features, n_features)\n",
    "    tol : float, default: 1e-4\n",
    "        Tolerance for stopping criteria.\n",
    "    random_state : int, RandomState instance or None, optional, default: None\n",
    "        The seed of the pseudo random number generator to use when shuffling\n",
    "        the data.  If int, random_state is the seed used by the random number\n",
    "        generator; If RandomState instance, random_state is the random number\n",
    "        generator; If None, the random number generator is the RandomState\n",
    "        instance used by `np.random`. Used when ``solver`` == 'sag' or\n",
    "        'liblinear'.\n",
    "    solver : str, {'auto', 'nelder-mead', 'cg', 'newton-cg'} default: 'auto'.\n",
    "        Algorithm to use in the optimization problem.\n",
    "        If 'auto', the choice will be determined by the supplied arguments.\n",
    "        - Nelder-Mead: neither Hessian nor Jacobian are provided\n",
    "        - CG: only Jacobian provided\n",
    "        - Newton-CG: both Jacobian and Hessian are provided\n",
    "    max_iter : int, default: 100\n",
    "        Useful only for the newton-cg, sag and lbfgs solvers.\n",
    "        Maximum number of iterations taken for the solvers to converge.\n",
    "    verbose : int, default: 0\n",
    "        For the liblinear and lbfgs solvers set verbose to any positive\n",
    "        number for verbosity.\n",
    "    warm_start : bool, default: False\n",
    "        When set to True, reuse the solution of the previous call to fit as\n",
    "        initialization, otherwise, just erase the previous solution.\n",
    "        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "        .. versionadded:: 0.17\n",
    "           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        The number of jobs to use for the computation. This will only provide\n",
    "        speedup for n_targets > 1 and sufficient large problems.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : array, shape (n_features, )\n",
    "        Estimated coefficients of a given Hamiltonian.\n",
    "    n_iter_ : array (1, )\n",
    "        Actual number of iterations.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ham=None, penalty='sd2', tol=1e-4, alpha=0.0, reg=None,\n",
    "                 random_state=None, solver='auto', max_iter=100,\n",
    "                 verbose=0, warm_start=False, n_jobs=None):\n",
    "\n",
    "        self.ham = ham\n",
    "        self.penalty = penalty\n",
    "        self.alpha = alpha\n",
    "        self.reg = reg\n",
    "        self.random_state = random_state\n",
    "        self.solver = solver\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.warm_start = warm_start\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit the model according to the given training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "            For trajectory data, X will be functions of particle configurations,\n",
    "            typically in the form of basis functions\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target vector relative to X.\n",
    "            For trajectory data, y will be configurational energies\n",
    "        sample_weight : array-like, shape (n_samples,) optional\n",
    "            Array of weights that are assigned to individual samples.\n",
    "            If not provided, then each sample is given unit weight.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(self.alpha, numbers.Number) or self.alpha < 0:\n",
    "            raise ValueError(\"Penalty term must be positive; got (alpha=%r)\"\n",
    "                             % self.alpha)\n",
    "        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:\n",
    "            raise ValueError(\"Maximum number of iteration must be positive;\"\n",
    "                             \" got (max_iter=%r)\" % self.max_iter)\n",
    "        if not isinstance(self.tol, numbers.Number) or self.tol < 0:\n",
    "            raise ValueError(\"Tolerance for stopping criteria must be \"\n",
    "                             \"positive; got (tol=%r)\" % self.tol)\n",
    "            \n",
    "        solver = _check_solver(self.solver, self.penalty, self.dual)\n",
    "\n",
    "        if solver in ['newton-cg']:\n",
    "            _dtype = [np.float64, np.float32]\n",
    "        else:\n",
    "            _dtype = np.float64\n",
    "\n",
    "        X, y = check_X_y(X, y, y_numeric=True)\n",
    "\n",
    "        if ((sample_weight is not None) and\n",
    "                np.atleast_1d(sample_weight).ndim > 1):\n",
    "            raise ValueError(\"Sample weights must be 1D array or scalar\")\n",
    "            \n",
    "        if self.warm_start:\n",
    "            warm_start_coef = getattr(self, 'coef_', None)\n",
    "        else:\n",
    "            warm_start_coef = None\n",
    "    \n",
    "        self.coef_ = list()\n",
    "        \n",
    "        # pre-optimize by (step-wise) energy-matching linear regression\n",
    "        if self.penalty == 'energy' or solver == 'newton-cg':\n",
    "            w0 = _linear_Hamiltonian_approx(X, y, self.alpha, DD, CD)\n",
    "            \n",
    "        if solver == 'newton-cg' and self.penalty == 'energy':\n",
    "            func = _energy_loss\n",
    "            grad = lambda x, *args: _energy_loss_and_grad(x, *args)[1]\n",
    "            hess = _energy_grad_hess\n",
    "\n",
    "        if solver == 'newton-cg' and self.penalty == 'sd2':\n",
    "            func = _sd2_loss\n",
    "            grad = lambda x, *args: _sd2_loss_and_grad(x, *args)[1]\n",
    "            hess = _sd2_grad_hess\n",
    "\n",
    "            args = (X, target, self.alpha, sample_weight)\n",
    "            self.coef_, n_iter = newton_cg(hess, func, grad, w0, args=args,\n",
    "                                     maxiter=self.max_iter, tol=self.tol)\n",
    "            \n",
    "def newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,\n",
    "              maxiter=100, maxinner=200, line_search=True, warn=True):\n",
    "\n",
    "        self.coef_, self.n_iter_ = ridge_regression(\n",
    "                X, y, alpha=self.alpha, sample_weight=sample_weight,\n",
    "                max_iter=self.max_iter, tol=self.tol, solver=self.solver,\n",
    "                random_state=self.random_state, return_n_iter=True,\n",
    "                return_intercept=False)\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Configurational energy predictions.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        T : array-like, shape = [n_samples,]\n",
    "            Returns the energy of n configurations.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"coef_\"):\n",
    "            raise NotFittedError(\"Call fit before prediction\")\n",
    "\n",
    "        return X.dot(self.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
